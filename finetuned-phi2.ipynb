{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:33:07.630545Z","iopub.execute_input":"2025-06-30T06:33:07.630896Z","iopub.status.idle":"2025-06-30T06:33:07.928002Z","shell.execute_reply.started":"2025-06-30T06:33:07.630849Z","shell.execute_reply":"2025-06-30T06:33:07.927370Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers.git --upgrade --quiet\n!pip install peft bitsandbytes accelerate datasets --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:31:21.433249Z","iopub.execute_input":"2025-06-30T06:31:21.433497Z","iopub.status.idle":"2025-06-30T06:33:07.628707Z","shell.execute_reply.started":"2025-06-30T06:31:21.433468Z","shell.execute_reply":"2025-06-30T06:33:07.627360Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:36:03.491079Z","iopub.execute_input":"2025-06-30T06:36:03.491372Z","iopub.status.idle":"2025-06-30T06:36:04.637148Z","shell.execute_reply.started":"2025-06-30T06:36:03.491349Z","shell.execute_reply":"2025-06-30T06:36:04.636389Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model_name = \"microsoft/phi-2\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:36:06.163035Z","iopub.execute_input":"2025-06-30T06:36:06.163347Z","iopub.status.idle":"2025-06-30T06:36:41.499359Z","shell.execute_reply.started":"2025-06-30T06:36:06.163323Z","shell.execute_reply":"2025-06-30T06:36:41.498569Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e83362d708a84c299024542a7804c12b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d29576393e4b4c27ac97005b15c07031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc0715d5b1245658cad9c12b718bbec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08353e7ce641469fb8c8714309034275"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dc0b6099fb54819834450b035809fc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75d84fe8db174344b7172c547b6d1dc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35da94f781de4f77b33f3498276a6126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3ccc2cf3e9453588a660e2d9ca374e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"327c94b69d13471f8d07a21b7083ef77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4df211fffe7146a69ebc6266906ab7f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b01bef05db34c6aa866e1a6e04b6ba0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ba3a6f561f466aa90241bd354f0704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed15bcd612b04d9ab96cc8c6a999de79"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:37:45.880839Z","iopub.execute_input":"2025-06-30T06:37:45.881592Z","iopub.status.idle":"2025-06-30T06:37:45.981298Z","shell.execute_reply.started":"2025-06-30T06:37:45.881554Z","shell.execute_reply":"2025-06-30T06:37:45.980477Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,621,440 || all params: 2,782,305,280 || trainable%: 0.0942\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"dataset = load_dataset(\"neil-code/dialogsum-test\", split=\"train\")  # Just a small test subset\n\ndef format(example):\n    return {\"text\": f\"Dialogue:\\n{example['dialogue']}\\n\\nTL;DR:\\n{example['summary']}\"}\n\ndataset = dataset.map(format)\n\ndef tokenize(example):\n    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n\ntokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:38:09.942198Z","iopub.execute_input":"2025-06-30T06:38:09.942513Z","iopub.status.idle":"2025-06-30T06:38:15.311937Z","shell.execute_reply.started":"2025-06-30T06:38:09.942491Z","shell.execute_reply":"2025-06-30T06:38:15.311092Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d422e4fe5084b6699081b39e850d974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01bc2462e7714b59a37f58f1b3223ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5234a5c7da304bbfab462e707a2c31af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab76f332993c4b429780381c77b9c57c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9e6317547649b8ab7270f2732006ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b047396b49ab4827aca52441120f98a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18aa80d46fd54eb0b4a6da4b9365f3cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3efc6a213a94e70b54b8a9681bbe70d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e4e2fa9442342d4a811d3001030a949"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"/kaggle/working/phi2-dialogsum\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=50,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=5,\n    save_strategy=\"no\",\n    report_to=\"none\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:38:26.461499Z","iopub.execute_input":"2025-06-30T06:38:26.461822Z","iopub.status.idle":"2025-06-30T06:38:26.490146Z","shell.execute_reply.started":"2025-06-30T06:38:26.461802Z","shell.execute_reply":"2025-06-30T06:38:26.489547Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_dataset,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:38:38.425446Z","iopub.execute_input":"2025-06-30T06:38:38.426037Z","iopub.status.idle":"2025-06-30T06:49:10.234601Z","shell.execute_reply.started":"2025-06-30T06:38:38.426015Z","shell.execute_reply":"2025-06-30T06:49:10.234005Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 10:18, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>1.845800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.902400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.772900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.649500</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.662200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.601200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.647500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.620500</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.569300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.550700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=50, training_loss=1.6821979427337646, metrics={'train_runtime': 631.2408, 'train_samples_per_second': 0.634, 'train_steps_per_second': 0.079, 'total_flos': 3257835454464000.0, 'train_loss': 1.6821979427337646, 'epoch': 0.2})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/phi2-lora-dialogsum\")\ntokenizer.save_pretrained(\"/kaggle/working/phi2-lora-dialogsum\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:49:22.206843Z","iopub.execute_input":"2025-06-30T06:49:22.207401Z","iopub.status.idle":"2025-06-30T06:49:22.349051Z","shell.execute_reply.started":"2025-06-30T06:49:22.207376Z","shell.execute_reply":"2025-06-30T06:49:22.348300Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/phi2-lora-dialogsum/tokenizer_config.json',\n '/kaggle/working/phi2-lora-dialogsum/special_tokens_map.json',\n '/kaggle/working/phi2-lora-dialogsum/vocab.json',\n '/kaggle/working/phi2-lora-dialogsum/merges.txt',\n '/kaggle/working/phi2-lora-dialogsum/added_tokens.json',\n '/kaggle/working/phi2-lora-dialogsum/tokenizer.json')"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import pipeline\n\n# Disable gradient checkpointing + re-enable caching\nmodel.gradient_checkpointing_disable()\nmodel.config.use_cache = True\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n\nprompt = \"Dialogue:\\nHi, how was your trip?\\nIt was amazing! We visited 3 new countries.\\n\\nTL;DR:\\n\"\noutputs = pipe(prompt, max_new_tokens=50, do_sample=True)\n\nprint(outputs[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:52:12.149266Z","iopub.execute_input":"2025-06-30T06:52:12.150034Z","iopub.status.idle":"2025-06-30T06:52:16.440007Z","shell.execute_reply.started":"2025-06-30T06:52:12.150011Z","shell.execute_reply":"2025-06-30T06:52:16.439286Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\nHi, how was your trip?\nIt was amazing! We visited 3 new countries.\n\nTL;DR:\nWe visited 3 new countries on our trip.\n\nTLDR:\nTLDR is an abbreviation for \"too long; didn't read.\" TL;DR is used when you don't have time to read a longer message. TLDRs\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}